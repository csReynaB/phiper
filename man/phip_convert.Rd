% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/standard-convert.R
\name{phip_convert}
\alias{phip_convert}
\title{Convert raw PhIP-Seq output into a \code{phip_data} object}
\usage{
phip_convert(
  data_long_path,
  sample_id = NULL,
  peptide_id = NULL,
  subject_id = NULL,
  timepoint = NULL,
  present = NULL,
  fold_change = NULL,
  counts_input = NULL,
  counts_hit = NULL,
  backend = "duckdb",
  n_cores = 8,
  materialise_table = TRUE,
  auto_expand = FALSE,
  peptide_library = TRUE
)
}
\arguments{
\item{data_long_path}{Character scalar. File or directory containing the
\emph{long-format} PhIP-Seq data. Allowed extensions are \strong{\code{.csv}} and
\strong{\code{.parquet}}. Directories are treated as partitions of a parquet set.}

\item{sample_id, peptide_id, subject_id, timepoint, present, fold_change, counts_input, counts_hit}{Optional character strings. Supply these only if your column names differ
from the defaults (\code{"sample_id"}, \code{"peptide_id"}, \code{"subject_id"},
\code{"timepoint"}, \code{"present"}, \code{"fold_change"}, \code{"counts_input"},
\code{"counts_hit"}). Each argument should contain the \emph{name} of the column in the
incoming data; \code{NULL} lets the default stand.}

\item{backend}{One of \code{"duckdb"} (default), \code{"arrow"}, or \code{"memory"}.
Determines where the converted table is stored:
\itemize{
\item \strong{duckdb} - in-process DuckDB database; fast and SQL-capable.
\item \strong{arrow} - Apache Arrow dataset on disk; good for
columnar analytics & inter-process sharing.
\item \strong{memory} - ordinary R tibble; simplest but limited by RAM.
}}

\item{n_cores}{Integer >= 1. Number of CPU threads DuckDB/Arrow may use while
reading and writing files. Ignored when \code{backend = "memory"}.}

\item{materialise_table}{Logical (DuckDB & Arrow only).
If \code{FALSE} the result is registered as a \strong{view}; if \code{TRUE} the table is
fully \strong{materialised} and stored on disk, trading higher load time and
storage for faster repeated queries.}

\item{auto_expand}{Logical. If \code{TRUE} and the incoming data are \strong{not} a
complete Cartesian product of \verb{sample_id x peptide_id}, missing
combinations are generated:
\itemize{
\item Columns that are constant within each \code{sample_id} (metadata) are copied
to the new rows.
\item Non-recyclable measurement columns (\code{fold_change}, \code{present},
\code{counts_input}, \code{counts_hit}, etc.) are initialised to 0.
The expanded table replaces the original \emph{in place}.
}}

\item{peptide_library}{Logical. If \code{TRUE} (default) \code{phip_convert()} will
attempt to locate and attach the matching peptide-library metadata for
downstream annotation. Set to \code{FALSE} to skip this step.}
}
\value{
An S3 object of class \strong{\code{phip_data}} containing:
\describe{
\item{\code{data_long}}{The (possibly expanded) long-format table, stored in the
selected back-end.}
\item{\code{comparisons}}{A tibble of pre-computed group comparisons or
\code{NULL} if none were supplied.}
\item{\code{peptide_library}}{Loaded peptide-library metadata (if
\code{peptide_library = TRUE}).}
\item{\code{backend}}{Character vector indicating the storage engine.}
\item{\code{meta}}{List with connection handles or temporary paths relevant to
the chosen backend.}
}
}
\description{
\code{phip_convert()} ingests a "long" table of PhIPsSeq read counts /
enrichment statistics, optionally expands it to the full
\verb{sample_id x peptide_id} grid, and registers the result in one of three
back-ends (\emph{DuckDB}, \emph{Arrow}, or in-memory tibble).
The function returns a fully initialised \strong{\code{phip_data}} object that can be
queried with the tidy API used throughout the package.
}
\details{
\emph{Paths are resolved to absolute form} before any work begins, and explicit
checks confirm existence as well as extension validity.
When \code{backend = "memory"} the parameters \code{n_cores} and
\code{materialise_table} are ignored and safely reset to \code{NULL}.
}
\examples{
\dontrun{
# Basic DuckDB import, auto-detecting default column names
phip_obj <- phip_convert(
  data_long_path = "data/phip_long.parquet",
  backend = "duckdb",
  n_cores = 4,
  materialise_table = TRUE
)

# Import a CSV, rename columns, keep everything in memory
phip_mem <- phip_convert(
  data_long_path = "data/phip_long.csv",
  sample_id      = "sample",
  peptide_id     = "pep",
  backend        = "memory"
)
}

}
\seealso{
\itemize{
\item \code{new_phip_data()} for the object constructor.
\item \code{dplyr::tbl()} to query DuckDB/Arrow tables lazily.
}
}
